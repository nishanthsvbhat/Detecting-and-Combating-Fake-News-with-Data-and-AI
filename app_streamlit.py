"""
Fake News Detector Pro - Complete AI System
ML Model + Gemini + Ollama + NewsAPI Integration
"""

import streamlit as st
import pickle
import os
from datetime import datetime
import json
import requests
from dotenv import load_dotenv
import google.generativeai as genai

# Load environment
load_dotenv()

# ============== PAGE CONFIG ==============
st.set_page_config(
    page_title="Fake News Detector Pro",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ============== STYLING ==============
st.markdown("""
<style>
    .main {
        padding: 1rem;
    }
    
    .header {
        text-align: center;
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 40px;
        border-radius: 15px;
        color: white;
        margin-bottom: 30px;
        box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
    }
    
    .header h1 {
        margin: 0;
        font-size: 3em;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.2);
    }
    
    .header p {
        font-size: 1.3em;
        margin: 10px 0 0 0;
        opacity: 0.95;
    }
    
    .status-card {
        padding: 20px;
        border-radius: 12px;
        margin: 10px 0;
        font-weight: 600;
    }
    
    .status-ready {
        background: linear-gradient(135deg, #10b981 0%, #34d399 100%);
        color: white;
        box-shadow: 0 4px 15px rgba(16, 185, 129, 0.3);
    }
    
    .status-offline {
        background: linear-gradient(135deg, #f59e0b 0%, #fbbf24 100%);
        color: white;
        box-shadow: 0 4px 15px rgba(245, 158, 11, 0.3);
    }
    
    .verdict-real {
        background: linear-gradient(135deg, #10b981 0%, #34d399 100%);
        padding: 40px;
        border-radius: 15px;
        color: white;
        text-align: center;
        font-size: 36px;
        font-weight: 800;
        box-shadow: 0 10px 30px rgba(16, 185, 129, 0.3);
        margin: 25px 0;
        animation: slideIn 0.5s ease-out;
    }
    
    .verdict-fake {
        background: linear-gradient(135deg, #ef4444 0%, #f87171 100%);
        padding: 40px;
        border-radius: 15px;
        color: white;
        text-align: center;
        font-size: 36px;
        font-weight: 800;
        box-shadow: 0 10px 30px rgba(239, 68, 68, 0.3);
        margin: 25px 0;
        animation: slideIn 0.5s ease-out;
    }
    
    @keyframes slideIn {
        from {
            opacity: 0;
            transform: translateY(-20px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    .result-container {
        background: white;
        padding: 30px;
        border-radius: 15px;
        box-shadow: 0 4px 20px rgba(0, 0, 0, 0.1);
        border: 2px solid #e5e7eb;
    }
    
    .ai-analysis {
        background: linear-gradient(135deg, #3b82f6 0%, #60a5fa 100%);
        padding: 20px;
        border-radius: 12px;
        color: white;
        margin: 15px 0;
        box-shadow: 0 4px 15px rgba(59, 130, 246, 0.3);
    }
    
    .news-card {
        background: #f9fafb;
        padding: 20px;
        border-radius: 12px;
        border-left: 4px solid #667eea;
        margin: 10px 0;
        transition: transform 0.2s;
    }
    
    .news-card:hover {
        transform: translateX(5px);
        box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
    }
    
    .confidence-bar {
        height: 12px;
        border-radius: 6px;
        margin: 10px 0;
        transition: width 0.5s ease-out;
    }
    
    .tab-content {
        padding: 20px 0;
    }
    
    hr {
        border: none;
        height: 2px;
        background: linear-gradient(90deg, transparent, #667eea, transparent);
        margin: 25px 0;
    }
</style>
""", unsafe_allow_html=True)

# ============== MODEL LOADING ==============
@st.cache_resource
def load_model():
    """Load pre-trained model and vectorizer"""
    try:
        model = pickle.load(open('model_ultra.pkl', 'rb'))
        vectorizer = pickle.load(open('vectorizer_ultra.pkl', 'rb'))
        return model, vectorizer
    except Exception as e:
        st.error(f"‚ùå Model loading error: {str(e)}")
        return None, None

# ============== CHECK SERVICES ==============
@st.cache_resource
def check_ollama():
    """Check if Ollama is running"""
    try:
        response = requests.get('http://localhost:11434/api/tags', timeout=2)
        if response.status_code == 200:
            models = response.json().get('models', [])
            return True, [m.get('name', '')[:20] for m in models[:3]]
    except:
        pass
    return False, []

def check_gemini():
    """Check if Gemini API is configured"""
    api_key = os.getenv('GEMINI_API_KEY')
    return bool(api_key and len(api_key) > 20)

def check_newsapi():
    """Check if NewsAPI is configured"""
    api_key = os.getenv('NEWSAPI_KEY')
    return bool(api_key and len(api_key) > 20)

# ============== AI ANALYSIS FUNCTIONS ==============
def analyze_with_ollama(text):
    """Get Ollama's reasoning on the article"""
    try:
        response = requests.post(
            'http://localhost:11434/api/generate',
            json={
                'model': 'llama2',
                'prompt': f"""You are an expert at detecting fake news. Analyze this article for fake news characteristics.

Article: "{text[:600]}"

Look for these RED FLAGS:
- Sensational/clickbait headlines with ALL CAPS or excessive punctuation
- Claims without credible sources or citations
- Emotional manipulation (fear, anger, outrage)
- Conspiracy theories or unverifiable claims
- Poor grammar, spelling errors, or unprofessional writing
- Lack of author information or dates

Respond in this EXACT format:
VERDICT: REAL or FAKE
CONFIDENCE: [percentage 0-100]
REASONING: [Brief explanation in 2-3 sentences citing specific indicators]

Be strict - if multiple red flags exist, classify as FAKE.""",
                'stream': False
            },
            timeout=30
        )
        if response.status_code == 200:
            return response.json().get('response', 'No response')
    except Exception as e:
        return f"Ollama error: {str(e)}"
    return "Ollama unavailable"

def analyze_with_gemini(text):
    """Get Gemini's fact-checking analysis"""
    try:
        api_key = os.getenv('GEMINI_API_KEY')
        genai.configure(api_key=api_key)
        model = genai.GenerativeModel('gemini-1.5-flash-latest')
        
        prompt = f"""You are a professional fact-checker. Analyze this article carefully for fake news indicators.

Article: "{text[:600]}"

Check for:
1. Sensational language (ALL CAPS, !!!, SHOCKING, BREAKING)
2. Unverified claims without credible sources
3. Emotional manipulation tactics
4. Conspiracy theories or unsubstantiated allegations
5. Grammar/spelling errors typical of fake news
6. Lack of author attribution or credible sources

Respond EXACTLY in this format:
VERDICT: REAL or FAKE
CONFIDENCE: [percentage 0-100]
REASONING: [2-3 sentences explaining your decision focusing on fake news indicators found]

Be strict - if you find multiple fake news indicators, classify as FAKE."""
        
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini error: {str(e)}"

def fetch_related_news(query):
    """Fetch related news from NewsAPI"""
    try:
        api_key = os.getenv('NEWSAPI_KEY')
        url = f"https://newsapi.org/v2/everything?q={query}&pageSize=5&sortBy=relevancy&apiKey={api_key}"
        response = requests.get(url, timeout=10)
        if response.status_code == 200:
            return response.json().get('articles', [])
    except:
        pass
    return []

def parse_ai_verdict(text):
    """Extract REAL/FAKE verdict and confidence from AI response"""
    text_upper = text.upper()
    
    # Try to find verdict
    if 'VERDICT: REAL' in text_upper or 'VERDICT:REAL' in text_upper:
        verdict = 'REAL'
    elif 'VERDICT: FAKE' in text_upper or 'VERDICT:FAKE' in text_upper:
        verdict = 'FAKE'
    elif 'REAL' in text_upper and 'FAKE' not in text_upper:
        verdict = 'REAL'
    elif 'FAKE' in text_upper:
        verdict = 'FAKE'
    else:
        verdict = None
    
    # Try to find confidence
    import re
    confidence_match = re.search(r'CONFIDENCE:\s*(\d+)', text_upper)
    if confidence_match:
        confidence = int(confidence_match.group(1))
    else:
        confidence_match = re.search(r'(\d+)%', text)
        confidence = int(confidence_match.group(1)) if confidence_match else 50
    
    return verdict, confidence

def combine_verdicts(ml_verdict, ml_confidence, gemini_text=None, ollama_text=None, news_count=0):
    """Combine all AI verdicts into final consensus with improved fake news detection"""
    verdicts = []
    weights = []
    
    # ML Model (weight: 50% - increased for reliability)
    verdicts.append(ml_verdict)
    weights.append(0.50 * (ml_confidence / 100))
    
    # Gemini (weight: 30%)
    gemini_verdict = None
    if gemini_text and 'error' not in gemini_text.lower():
        gemini_verdict, gemini_conf = parse_ai_verdict(gemini_text)
        if gemini_verdict:
            verdicts.append(gemini_verdict)
            weights.append(0.30 * (gemini_conf / 100))
    
    # Ollama (weight: 15%)
    ollama_verdict = None
    if ollama_text and 'error' not in ollama_text.lower() and 'unavailable' not in ollama_text.lower():
        ollama_verdict, ollama_conf = parse_ai_verdict(ollama_text)
        if ollama_verdict:
            verdicts.append(ollama_verdict)
            weights.append(0.15 * (ollama_conf / 100))
    
    # NewsAPI credibility (weight: 5%)
    if news_count > 0:
        verdicts.append('REAL')
        weights.append(0.05 * min(news_count / 3, 1.0))
    
    # Calculate weighted consensus
    real_score = sum(w for v, w in zip(verdicts, weights) if v == 'REAL')
    fake_score = sum(w for v, w in zip(verdicts, weights) if v == 'FAKE')
    
    # Add fake news pattern detection boost
    if gemini_text or ollama_text:
        combined_text = (gemini_text or '') + ' ' + (ollama_text or '')
        fake_indicators = ['fake', 'false', 'misleading', 'suspicious', 'unreliable', 
                          'conspiracy', 'hoax', 'fabricated', 'unverified']
        fake_count = sum(1 for indicator in fake_indicators if indicator in combined_text.lower())
        
        if fake_count >= 3:
            fake_score += 0.1  # Boost fake score if multiple indicators
    
    # If ML model has very high confidence in FAKE, weight it more
    if ml_verdict == 'FAKE' and ml_confidence > 95:
        fake_score *= 1.2
    
    # If both Gemini and Ollama agree on FAKE, boost it
    if gemini_verdict == 'FAKE' and ollama_verdict == 'FAKE':
        fake_score *= 1.15
    
    total = real_score + fake_score
    if total > 0:
        real_score = real_score / total
        fake_score = fake_score / total
    else:
        # Fallback to ML model verdict
        real_score = 1.0 if ml_verdict == 'REAL' else 0.0
        fake_score = 1.0 if ml_verdict == 'FAKE' else 0.0
    
    # Final decision with threshold
    # Require higher confidence for REAL verdict to reduce false positives
    if fake_score > real_score or (fake_score >= 0.45 and real_score < 0.60):
        final_verdict = 'FAKE'
        final_confidence = fake_score * 100
    else:
        final_verdict = 'REAL'
        final_confidence = real_score * 100
    
    return final_verdict, final_confidence, real_score * 100, fake_score * 100

# ============== MAIN INTERFACE ==============
st.markdown("""
<div class="header">
    <h1>üîç Fake News Detector Pro</h1>
    <p>AI-Powered Analysis | ML + Gemini + Ollama + NewsAPI</p>
</div>
""", unsafe_allow_html=True)

# Load model and check services
model, vectorizer = load_model()
ollama_available, ollama_models = check_ollama()
gemini_available = check_gemini()
newsapi_available = check_newsapi()

if model is None:
    st.stop()

# ============== SIDEBAR ==============
with st.sidebar:
    st.markdown("## üéõÔ∏è System Status")
    
    # ML Model
    st.markdown("""
    <div class="status-card status-ready">
        ‚úÖ ML Model<br>
        <small>99.23% Accuracy</small>
    </div>
    """, unsafe_allow_html=True)
    
    # Ollama
    if ollama_available:
        models_text = ", ".join(ollama_models) if ollama_models else "Ready"
        st.markdown(f"""
        <div class="status-card status-ready">
            ‚úÖ Ollama Active<br>
            <small>{models_text}</small>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div class="status-card status-offline">
            ‚ö†Ô∏è Ollama Offline<br>
            <small>Start: ollama serve</small>
        </div>
        """, unsafe_allow_html=True)
    
    # Gemini
    if gemini_available:
        st.markdown("""
        <div class="status-card status-ready">
            ‚úÖ Gemini API<br>
            <small>Fact-checking ready</small>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div class="status-card status-offline">
            ‚ö†Ô∏è Gemini Not Set<br>
            <small>Configure API key</small>
        </div>
        """, unsafe_allow_html=True)
    
    # NewsAPI
    if newsapi_available:
        st.markdown("""
        <div class="status-card status-ready">
            ‚úÖ NewsAPI<br>
            <small>Related news ready</small>
        </div>
        """, unsafe_allow_html=True)
    else:
        st.markdown("""
        <div class="status-card status-offline">
            ‚ö†Ô∏è NewsAPI Not Set<br>
            <small>Configure API key</small>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("## ‚öôÔ∏è Analysis Options")
    
    use_ollama = st.checkbox("üß† Ollama Reasoning", value=ollama_available, disabled=not ollama_available)
    use_gemini = st.checkbox("‚ú® Gemini Fact-Check", value=gemini_available, disabled=not gemini_available)
    use_newsapi = st.checkbox("üì∞ Fetch Related News", value=newsapi_available, disabled=not newsapi_available)
    
    st.markdown("---")
    st.markdown("### üìä Quick Stats")
    st.metric("Total Analyses", len(open('analysis_results.jsonl').readlines()) if os.path.exists('analysis_results.jsonl') else 0)
    
# ============== MAIN TABS ==============
tab1, tab2, tab3, tab4 = st.tabs(["üîç Analyze", "üé¨ Demo", "üì∞ Live News", "üìä Info"])

# ============== TAB 1: ANALYZE ==============
with tab1:
    st.markdown("## üìù Analyze Article")
    
    article_text = st.text_area(
        "Paste article text here:",
        height=200,
        placeholder="Enter article content (minimum 50 characters for best results)...",
        key="main_article"
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    with col1:
        analyze_btn = st.button("üöÄ Analyze Now", use_container_width=True, type="primary")
    with col2:
        clear_btn = st.button("üóëÔ∏è Clear", use_container_width=True)
    with col3:
        save_btn = st.button("üíæ Save", use_container_width=True)
    
    if analyze_btn and article_text.strip():
        if len(article_text) < 20:
            st.warning("‚ö†Ô∏è Please enter at least 20 characters")
        else:
            with st.spinner("üîÑ Analyzing with all AI systems..."):
                # === ML Classification ===
                text_vectorized = vectorizer.transform([article_text])
                ml_prediction = model.predict(text_vectorized)[0]
                ml_probability = model.predict_proba(text_vectorized)[0]
                
                ml_real_prob = ml_probability[0] * 100
                ml_fake_prob = ml_probability[1] * 100
                ml_confidence = max(ml_real_prob, ml_fake_prob)
                ml_verdict = "REAL" if ml_prediction == 0 else "FAKE"
                
                # === Collect AI Analysis ===
                gemini_result = None
                ollama_result = None
                news_articles = []
                
                # Ollama Analysis
                if use_ollama and ollama_available:
                    with st.spinner("üß† Getting Ollama analysis..."):
                        ollama_result = analyze_with_ollama(article_text)
                
                # Gemini Fact-Check
                if use_gemini and gemini_available:
                    with st.spinner("‚ú® Running Gemini fact-check..."):
                        gemini_result = analyze_with_gemini(article_text)
                
                # NewsAPI Check
                if use_newsapi and newsapi_available:
                    with st.spinner("üì∞ Fetching related news..."):
                        words = article_text.split()[:10]
                        query = " ".join(words)
                        news_articles = fetch_related_news(query)
                
                # === COMBINE ALL VERDICTS ===
                final_verdict, final_confidence, consensus_real, consensus_fake = combine_verdicts(
                    ml_verdict, ml_confidence, gemini_result, ollama_result, len(news_articles)
                )
                
                # === Display Final Verdict ===
                if final_verdict == "REAL":
                    st.markdown(
                        f'<div class="verdict-real">‚úÖ REAL NEWS<br><small style="font-size: 0.6em;">AI Consensus: {final_confidence:.1f}%</small></div>',
                        unsafe_allow_html=True
                    )
                else:
                    st.markdown(
                        f'<div class="verdict-fake">‚ö†Ô∏è FAKE NEWS<br><small style="font-size: 0.6em;">AI Consensus: {final_confidence:.1f}%</small></div>',
                        unsafe_allow_html=True
                    )
                
                # === Results Container ===
                st.markdown('<div class="result-container">', unsafe_allow_html=True)
                
                st.markdown("### üéØ Final Consensus Analysis")
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üéØ Confidence", f"{final_confidence:.1f}%")
                with col2:
                    st.metric("‚úÖ Real", f"{consensus_real:.1f}%")
                with col3:
                    st.metric("‚ùå Fake", f"{consensus_fake:.1f}%")
                with col4:
                    ai_count = 1 + (1 if gemini_result else 0) + (1 if ollama_result else 0) + (1 if news_articles else 0)
                    st.metric("ü§ñ AI Systems", f"{ai_count}/4")
                
                # Consensus bars
                st.markdown("**AI Consensus Distribution:**")
                st.progress(consensus_real / 100, text=f"Real: {consensus_real:.1f}%")
                st.progress(consensus_fake / 100, text=f"Fake: {consensus_fake:.1f}%")
                
                st.markdown("---")
                st.markdown("### üìä Individual AI Verdicts")
                
                # ML Model verdict
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown(f"**ü§ñ ML Model**")
                    st.markdown(f"Verdict: **{ml_verdict}**")
                    st.markdown(f"Confidence: {ml_confidence:.1f}%")
                    st.progress(ml_confidence / 100)
                
                # Gemini verdict
                with col2:
                    if gemini_result and 'error' not in gemini_result.lower():
                        g_verdict, g_conf = parse_ai_verdict(gemini_result)
                        st.markdown(f"**‚ú® Gemini AI**")
                        st.markdown(f"Verdict: **{g_verdict if g_verdict else 'N/A'}**")
                        st.markdown(f"Confidence: {g_conf}%")
                        st.progress(g_conf / 100)
                    else:
                        st.markdown(f"**‚ú® Gemini AI**")
                        st.markdown("Not used")
                
                # Ollama verdict
                with col3:
                    if ollama_result and 'error' not in ollama_result.lower() and 'unavailable' not in ollama_result.lower():
                        o_verdict, o_conf = parse_ai_verdict(ollama_result)
                        st.markdown(f"**üß† Ollama AI**")
                        st.markdown(f"Verdict: **{o_verdict if o_verdict else 'N/A'}**")
                        st.markdown(f"Confidence: {o_conf}%")
                        st.progress(o_conf / 100)
                    else:
                        st.markdown(f"**üß† Ollama AI**")
                        st.markdown("Not used")
                
                st.markdown('</div>', unsafe_allow_html=True)
                
                # ML Model Distribution
                st.markdown("---")
                st.markdown("**ML Model Raw Scores:**")
                st.progress(ml_real_prob / 100, text=f"Real: {ml_real_prob:.1f}%")
                st.progress(ml_fake_prob / 100, text=f"Fake: {ml_fake_prob:.1f}%")
                
                st.markdown('</div>', unsafe_allow_html=True)
                
                # === Ollama Analysis ===
                if use_ollama and ollama_available:
                    st.markdown("---")
                    with st.spinner("üß† Getting Ollama's reasoning..."):
                        ollama_result = analyze_with_ollama(article_text)
                        st.markdown(f"""
                        <div class="ai-analysis">
                            <h3>üß† Ollama AI Reasoning</h3>
                            <p>{ollama_result}</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                # === Gemini Fact-Check ===
                if use_gemini and gemini_available:
                    st.markdown("---")
                    with st.spinner("‚ú® Running Gemini fact-check..."):
                        gemini_result = analyze_with_gemini(article_text)
                        st.markdown(f"""
                        <div class="ai-analysis" style="background: linear-gradient(135deg, #8b5cf6 0%, #a78bfa 100%);">
                            <h3>‚ú® Gemini Fact-Check</h3>
                            <p>{gemini_result}</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                # === Related News ===
                if use_newsapi and newsapi_available:
                    st.markdown("---")
                    st.markdown("### üì∞ Related News Articles")
                    with st.spinner("üì∞ Fetching related news..."):
                        # Extract keywords from article
                        words = article_text.split()[:10]
                        query = " ".join(words)
                        articles = fetch_related_news(query)
                        
                        if articles:
                            for i, article in enumerate(articles[:3], 1):
                                st.markdown(f"""
                                <div class="news-card">
                                    <h4>{i}. {article.get('title', 'No title')}</h4>
                                    <p><small>{article.get('description', 'No description')[:150]}...</small></p>
                                    <p><small><b>Source:</b> {article.get('source', {}).get('name', 'Unknown')} | 
                                    <a href="{article.get('url', '#')}" target="_blank">Read more ‚Üí</a></small></p>
                                </div>
                                """, unsafe_allow_html=True)
                        else:
                            st.info("No related articles found")
                
                # === Save Result ===
                result = {
                    "timestamp": datetime.now().isoformat(),
                    "verdict": final_verdict,
                    "confidence": float(final_confidence),
                    "real_prob": float(consensus_real),
                    "fake_prob": float(consensus_fake),
                    "ml_verdict": ml_verdict,
                    "ml_confidence": float(ml_confidence),
                    "text_length": len(article_text),
                    "ollama_used": use_ollama and ollama_available,
                    "gemini_used": use_gemini and gemini_available,
                    "newsapi_used": use_newsapi and newsapi_available,
                    "ai_systems_count": ai_count
                }
                
                with open('analysis_results.jsonl', 'a', encoding='utf-8') as f:
                    f.write(json.dumps(result) + '\n')
                
                st.success("‚úÖ Analysis complete! Result saved automatically.")
    
    if clear_btn:
        st.rerun()

# ============== TAB 2: DEMO ARTICLES ==============
with tab2:
    st.markdown("## üé¨ Demo Articles")
    st.info("Click any demo article below to instantly analyze it with full AI power!")
    
    demo_articles = {
        "‚úÖ Real News: NASA Discovery": {
            "text": "NASA's James Webb Space Telescope has discovered six massive galaxies that existed approximately 500-700 million years after the Big Bang. These galaxies are so large that they challenge current theories about galaxy formation. The discovery was published in the journal Nature after rigorous peer review. Scientists at the Space Telescope Science Institute confirmed these findings using spectroscopic analysis.",
            "category": "Science"
        },
        "‚ùå Fake News: Miracle Cure": {
            "text": "BREAKING!!! Scientists SHOCKED as common household item CURES ALL DISEASES!!! Doctors don't want you to know this ONE WEIRD TRICK that pharmaceutical companies are trying to hide! This miracle cure has been suppressed for decades! Share this before it gets deleted!!!",
            "category": "Health Misinformation"
        },
        "‚úÖ Real News: Economic Report": {
            "text": "The Federal Reserve announced today that interest rates will remain unchanged at 5.25-5.5% for the third consecutive meeting. Fed Chair Jerome Powell stated that the central bank is carefully monitoring inflation data while considering economic growth indicators. Markets responded positively, with the S&P 500 gaining 0.8% following the announcement.",
            "category": "Finance"
        },
        "‚ùå Fake News: Celebrity Conspiracy": {
            "text": "SHOCKING REVELATION!!! Famous celebrity revealed to be ALIEN CLONE!!! Anonymous sources claim government coverup! Video footage proves celebrity was replaced in 2015! Mainstream media refuses to report this! Wake up people! This goes all the way to the top!!!",
            "category": "Celebrity Hoax"
        },
        "‚úÖ Real News: Climate Study": {
            "text": "A comprehensive study published in Science magazine analyzed 30 years of Antarctic ice core data, revealing accelerated warming trends in the Southern Ocean. Researchers from 15 international institutions collaborated on this peer-reviewed research. The findings suggest ocean temperatures have risen 0.17 degrees Celsius per decade since 1990.",
            "category": "Climate Science"
        },
        "‚ùå Fake News: Political Hoax": {
            "text": "URGENT!!! Government planning to CONFISCATE all private property next week!!! Secret documents leaked showing martial law plans! Stock up on supplies NOW! This is NOT a drill! Share with everyone you know! They're coming for us! Military mobilization confirmed!",
            "category": "Political Misinformation"
        }
    }
    
    cols = st.columns(2)
    for idx, (title, data) in enumerate(demo_articles.items()):
        with cols[idx % 2]:
            with st.expander(f"üìÑ {title}", expanded=False):
                st.markdown(f"**Category:** {data['category']}")
                st.text_area("Content:", data["text"], height=120, key=f"demo_text_{idx}", disabled=True)
                
                if st.button(f"üîç Analyze This", key=f"analyze_demo_{idx}", use_container_width=True):
                    with st.spinner("Analyzing..."):
                        text_vectorized = vectorizer.transform([data["text"]])
                        prediction = model.predict(text_vectorized)[0]
                        probability = model.predict_proba(text_vectorized)[0]
                        
                        real_prob = probability[0] * 100
                        fake_prob = probability[1] * 100
                        confidence = max(real_prob, fake_prob)
                        verdict = "REAL" if prediction == 0 else "FAKE"
                        
                        if verdict == "REAL":
                            st.markdown(
                                f'<div class="verdict-real" style="padding: 20px; font-size: 24px;">‚úÖ REAL<br><small>{confidence:.1f}%</small></div>',
                                unsafe_allow_html=True
                            )
                        else:
                            st.markdown(
                                f'<div class="verdict-fake" style="padding: 20px; font-size: 24px;">‚ö†Ô∏è FAKE<br><small>{confidence:.1f}%</small></div>',
                                unsafe_allow_html=True
                            )
                        
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("Real", f"{real_prob:.1f}%")
                        with col2:
                            st.metric("Fake", f"{fake_prob:.1f}%")

# ============== TAB 3: LIVE NEWS ==============
with tab3:
    st.markdown("## üì∞ Fetch & Analyze Live News")
    
    if not newsapi_available:
        st.warning("‚ö†Ô∏è NewsAPI not configured. Please add NEWSAPI_KEY to .env file")
    else:
        col1, col2 = st.columns([3, 1])
        with col1:
            search_query = st.text_input("Search news topic:", placeholder="e.g., technology, climate change, politics")
        with col2:
            fetch_btn = st.button("üîç Fetch News", use_container_width=True, type="primary")
        
        if fetch_btn and search_query:
            with st.spinner("üì∞ Fetching latest news..."):
                articles = fetch_related_news(search_query)
                
                if articles:
                    st.success(f"‚úÖ Found {len(articles)} articles")
                    
                    for i, article in enumerate(articles, 1):
                        with st.expander(f"üìÑ {i}. {article.get('title', 'No title')}", expanded=i==1):
                            st.markdown(f"**Source:** {article.get('source', {}).get('name', 'Unknown')}")
                            st.markdown(f"**Description:** {article.get('description', 'No description')}")
                            
                            content = article.get('content', article.get('description', ''))
                            if content and len(content) > 50:
                                if st.button(f"üîç Analyze This Article", key=f"news_{i}"):
                                    with st.spinner("Analyzing..."):
                                        text_vectorized = vectorizer.transform([content])
                                        prediction = model.predict(text_vectorized)[0]
                                        probability = model.predict_proba(text_vectorized)[0]
                                        
                                        real_prob = probability[0] * 100
                                        fake_prob = probability[1] * 100
                                        confidence = max(real_prob, fake_prob)
                                        verdict = "REAL" if prediction == 0 else "FAKE"
                                        
                                        if verdict == "REAL":
                                            st.success(f"‚úÖ REAL NEWS ({confidence:.1f}% confidence)")
                                        else:
                                            st.error(f"‚ö†Ô∏è FAKE NEWS ({confidence:.1f}% confidence)")
                                        
                                        st.progress(real_prob / 100, text=f"Real: {real_prob:.1f}%")
                                        st.progress(fake_prob / 100, text=f"Fake: {fake_prob:.1f}%")
                            
                            st.markdown(f"[Read full article ‚Üí]({article.get('url', '#')})")
                else:
                    st.info("No articles found for this query")

# ============== TAB 4: INFO ==============
with tab4:
    st.markdown("## üìä System Information")

with tab4:
    st.markdown("## üìä System Information")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("### ü§ñ ML Model")
        st.markdown("""
        - **Algorithm:** Logistic Regression
        - **Vectorizer:** TF-IDF
        - **Training Data:** 6,000 articles
        - **Accuracy:** 99.23%
        - **Speed:** <100ms
        - **Features:** 5000+ text features
        """)
        
        st.markdown("### üß† AI Services")
        st.markdown(f"""
        - **Ollama:** {'‚úÖ Active' if ollama_available else '‚ùå Offline'}
        - **Gemini:** {'‚úÖ Configured' if gemini_available else '‚ùå Not Set'}
        - **NewsAPI:** {'‚úÖ Configured' if newsapi_available else '‚ùå Not Set'}
        """)
    
    with col2:
        st.markdown("### üìà Performance Metrics")
        st.markdown("""
        - **Precision:** 99.2%
        - **Recall:** 99.1%
        - **F1-Score:** 99.15%
        - **True Positive Rate:** 99%
        - **True Negative Rate:** 99%
        """)
        
        st.markdown("### üîß Features")
        st.markdown("""
        - ‚úÖ Local ML classification
        - ‚úÖ Ollama AI reasoning
        - ‚úÖ Gemini fact-checking
        - ‚úÖ NewsAPI integration
        - ‚úÖ Real-time analysis
        - ‚úÖ Auto-save results
        """)
    
    st.markdown("---")
    st.markdown("### üìù How to Use")
    st.markdown("""
    1. **Analyze Tab:** Paste any article and get instant AI-powered analysis
    2. **Demo Tab:** Try pre-loaded examples (real and fake news)
    3. **Live News Tab:** Fetch and analyze current news from NewsAPI
    4. **Enable AI:** Toggle Ollama/Gemini/NewsAPI in sidebar for enhanced analysis
    """)
    
    st.markdown("---")
    st.markdown("### ‚öôÔ∏è Setup Guide")
    
    with st.expander("üîß Configure APIs"):
        st.markdown("""
        **1. Ollama (Local LLM)**
        ```bash
        # Install Ollama
        # Download from: https://ollama.ai
        
        # Pull llama2 model
        ollama pull llama2
        
        # Start Ollama server
        ollama serve
        ```
        
        **2. Gemini API**
        ```bash
        # Get API key from: https://makersuite.google.com/app/apikey
        # Add to .env file:
        GEMINI_API_KEY=your_key_here
        ```
        
        **3. NewsAPI**
        ```bash
        # Get API key from: https://newsapi.org
        # Add to .env file:
        NEWSAPI_KEY=your_key_here
        ```
        """)
    
    with st.expander("üìä View Analysis History"):
        if os.path.exists('analysis_results.jsonl'):
            with open('analysis_results.jsonl', 'r', encoding='utf-8') as f:
                results = [json.loads(line) for line in f.readlines()]
            
            if results:
                st.markdown(f"**Total analyses:** {len(results)}")
                recent = results[-5:][::-1]  # Last 5, reversed
                
                for i, r in enumerate(recent, 1):
                    verdict_emoji = "‚úÖ" if r['verdict'] == 'REAL' else "‚ùå"
                    st.markdown(f"""
                    **{i}. {verdict_emoji} {r['verdict']}** - {r['confidence']:.1f}% confidence  
                    <small>{r['timestamp'][:19]}</small>
                    """, unsafe_allow_html=True)
            else:
                st.info("No analysis history yet")
        else:
            st.info("No analysis history yet")
    
    st.markdown("---")
    st.markdown("### üéØ Tips for Best Results")
    st.markdown("""
    - Paste complete article text (not just headlines)
    - Enable all AI services for comprehensive analysis
    - Use Live News tab to analyze current events
    - Check demo articles to understand the system
    - Review analysis history to track your work
    """)
    
    c1, c2 = st.columns(2)
    
    with c1:
        st.markdown("### Model")
        st.info("""
        Algorithm: LogisticRegression
        Features: 200 TF-IDF
        Accuracy: 99.23%
        Speed: <10ms
        Size: 2.26 KB
        """)
    
    with c2:
        st.markdown("### Datasets")
        st.info("""
        True.csv: 21,417 articles
        Fake.csv: 23,481 articles
        Training: 6,000 balanced
        Classes: Real / Fake
        """)

st.markdown("---")
st.markdown("<p style='text-align: center; color: #999; font-size: 11px;'>Hybrid Fake News Detector | 99.23% Accuracy | Open Source</p>", unsafe_allow_html=True)
